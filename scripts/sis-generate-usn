#!/usr/bin/env python
# Copyright 2007 Kees Cook <kees@ubuntu.com>
# License: GPLv2
# Goes and finds all the files that make up a set of .changes files.
import sys, os, os.path, tempfile, subprocess, re
import apt_pkg, gzip
import optparse
import cve_lib

import warnings
warnings.filterwarnings('ignore', message=r'.*apt_pkg\.TagFile.*', category=DeprecationWarning)

opter = optparse.OptionParser()
opter.add_option("--debug", help="Verbose processing output", action='store_true')
opter.add_option("--no-new-warn", help="Do not warn about NEW binaries", action='store_true')
(opt, args) = opter.parse_args()

if len(args) < 2:
    raise ValueError, "Must give USN and all changes files"

official_architectures = ['source', 'all', 'amd64', 'i386', 'lpia', 'powerpc', 'sparc']
releasemap = {
    'dapper-security': 'dapper',
    'gutsy-security': 'gutsy',
    'hardy-security': 'hardy',
    'intrepid-security': 'intrepid',
    'jaunty-security': 'jaunty',
    'karmic-security': 'karmic',
    'lucid-security': 'lucid',
}
components = ['main','restricted','universe','multiverse']
# Put -security first since it tends to have more correctly-validate
# components (-updates has gotten wrecked by things in -proposed before).
release_suffixes = ['','-security','-updates']

config = cve_lib.read_config()
archive = config['packages_mirror']
pool = '%s/pool' % (archive)
dists = '%s/dists' % (archive)
published = 'http://security.ubuntu.com/ubuntu/pool/'
ports     = 'http://ports.ubuntu.com/pool/'

bin_locations = dict()
src_locations = dict()

if not os.path.exists(dists):
    print >>sys.stderr, "Could not find '%s'" % (dists)
    sys.exit(1)

for release in releasemap.values():
    def scan_packages(locations, component, release, arch, name="Packages", di=False):
        '''Used to locate which component a given source package and
           related binary packages live'''
        archloc = arch
        if archloc != 'source':
            archloc = 'binary-%s' % (arch)
        # It seems that the d-i files have a separate name-space ('-di_*.udeb')
        # so it is safe to merge them together on a per-release basis.
        if di:
            dipath = 'debian-installer/'
        else:
            dipath = ""
        filename = '%s/%s/%s/%s%s/%s' % (dists, release, component, dipath, archloc, name)
        #print >>sys.stderr, "Loading %s" % (filename)
        if os.path.exists(filename):
            handle = open(filename)
        else:
            filename += ".gz"
            if not os.path.exists(filename):
                #print >>sys.stderr, "WARN: %s does not exist" % (filename)
                return
            handle = tempfile.TemporaryFile()
            handle.write(gzip.open(filename).read())
            handle.seek(0)
        #print >>sys.stderr, "INFO: loading %s" % (filename)
        parser = apt_pkg.ParseTagFile(handle)
        locations.setdefault(release, dict())
        locations[release].setdefault(arch, dict())
        while True:
            parser.Step()
            s = parser.Section
            if not s:
                break
            pkg = s['Package']
            pkgcomponent = component
            try:
                if '/' in s['Section']:
                    pkgcomponent = s['Section'].split('/')[0]
            except:
                pass
                #print >>sys.stderr, "%s(%s): " % (filename, pkg) + " ".join(s.keys())
                #raise
            if locations[release][arch].has_key(pkg):
                msg = "Duplicate %s for %s in %s (%s and %s)" % (archloc, pkg, release, locations[release][arch][pkg], pkgcomponent)
                print >>sys.stderr, msg
            locations[release][arch].setdefault(pkg, pkgcomponent)

    for suffix in release_suffixes:
        for component in components:
            for arch in official_architectures:
                if arch == 'source' or arch == 'all':
                    continue
                scan_packages(bin_locations, component, release+suffix, arch)
                scan_packages(bin_locations, component, release+suffix, arch, di=True)
            scan_packages(src_locations, component, release+suffix, "source", name="Sources")

def open_gpg_clearsign(path):
    '''Open a GPG-signed .changes file and strip off the GPG envelope, so that
    it becomes suitable for apt_pkg parsing. Returns a file object.'''

    dest = tempfile.TemporaryFile()
    f = open(path)

    line = f.readline()
    clearsigned = False
    if line.startswith('-----BEGIN PGP SIGNED MESSAGE-----'):
        clearsigned = True
        for line in f:
            if line.strip() == '':
                break
    else:
        dest.write(line)

    for line in f:
        if clearsigned and line.find('BEGIN PGP SIGNATURE') >= 0:
            break
        dest.write(line)

    f.close()
    dest.seek(0)
    return dest

def parse_CVEs(text):
    '''Return a set of all CVEs mentioned in the given text.'''

    result = set([])
    cvere = re.compile("((?:CAN|can|CVE|cve)-\d\d\d\d-\d\d\d\d)")
    for cve in cvere.finditer(text):
        result.add(cve.group().upper().replace('CAN', 'CVE', 1))
    return result

def find_archive_path_one_release(source, release, arch, filename, fuzzy=None):
    '''Try to find the local archive path for a given .deb filename, release, and architecture'''
    # When performing filename analysis for the component-match,
    # allow for a different filename (for handling guesses at NEW names).
    # This is distinct from "filename" so that the resulting archive path
    # contains the actual expected filename (we just do bin_location[]
    # look-ups with the "fuzzy" filename.
    if not fuzzy:
        fuzzy = filename

    if source.startswith('lib'):
        prefix = source[:4]
    else:
        prefix = source[:1]

    try:
        bin, version, archdeb = fuzzy.split('_',2)
    except:
        # non-deb: stored in the source package's location
        try:
            base = os.path.join(src_locations[release][arch][source], prefix, source, filename)
        except KeyError, e:
            raise KeyError, "cannot find %s path for %s %s/%s/%s" % (arch, release, prefix, source, filename)
        filepath = os.path.join(pool, base)
        return filepath, base

    # deb: stored in the binary package's location
    if arch == 'all':
        arch = 'i386'
    try:
        base = os.path.join(bin_locations[release][arch][bin], prefix, source, filename)
    except KeyError, e:
        failure = str(e)
        if failure == release:
            raise KeyError, "cannot find release '%s'" % (release)
        elif failure == arch:
            raise KeyError, "cannot find arch '%s' in release '%s'" % (arch, release)
        elif failure == bin:
            raise KeyError, "cannot find binary '%s' (arch '%s') in release '%s'" % (bin, arch, release)
        raise KeyError, "unknown KeyError '%s'" % (failure)
    filepath = os.path.join(pool, base)
    return filepath, base

def find_archive_path_each_release(source, release, arch, filename, fuzzy=None):
    '''Try each release to find an archive path'''
    msgs = []
    for suffix in release_suffixes:
        try:
            filepath, base = find_archive_path_one_release(source, release+suffix, arch, filename, fuzzy)
            return filepath, base
        except KeyError, e:
            msgs += [str(e)]
            pass
    raise KeyError, "\n".join(msgs)

def find_archive_path(source, release, arch, filename):
    try:
        filepath, base = find_archive_path_each_release(source, release, arch, filename)
    except KeyError, e:
        # Try to guess for an incremented NEW binary name
        found = False
        try:
            if filename.endswith('deb'):
                if opt.debug:
                    print >>sys.stderr, "missing: " + filename
                bin, version, archdeb = filename.split('_',2)
                # skip leading numbers (i.e. the kernel version)
                match = re.search(r'[^\d]+(?:-[\d\.]+)?-(\d+)', bin)
                if not match:
                    # Handle incrementing libraries (i.e. libdnsXX)
                    match = re.search(r'[^\d]+(\d+)', bin)
                if match:
                    if opt.debug:
                        print >>sys.stderr, "guessing abi: " + match.group(1)
                    inc = int(match.group(1)) - 1
                    while not found and inc > 0:
                        # Attempt to locate a binary with a similar name.
                        # I.e. if it ends with a number, find the same filename
                        # but with the number part reduced by 1.
                        newbin = match.string[:match.start(1)] + '%d' % (inc) + match.string[match.end(1):]
                        if opt.debug:
                            print >>sys.stderr, "trying: " + newbin
                        fuzzy_filename = "%s_%s_%s" % (newbin, version, archdeb)
                        try:
                            filepath, base = find_archive_path_each_release(source, release, arch, filename, fuzzy_filename)
                            found = True
                            if not opt.no_new_warn:
                                print >>sys.stderr, "WARN: found '%s' in place of NEW '%s'" % (newbin, bin)
                        except:
                            inc -= 1
                            pass
        except:
            pass
        if not found:
            raise e

    return filepath, base

def base_to_url(base, release, arch):
    if arch == 'lpia':
        return os.path.join(ports,base)
    if arch in ['powerpc','sparc'] and release not in ['dapper','gutsy']:
        return os.path.join(ports,base)
    return os.path.join(published,base)

# This routine isn't useful for finding the expected download locations of debs
def lp_find_url(source, srcver, filename, release, arch):
    try:
        bin, version, archdeb = filename.split('_',2)
    except:
        return 'https://launchpad.net/ubuntu/%s/+source/%s/%s/+files/%s' % (release, source, srcver, filename)
    return 'https://launchpad.net/ubuntu/%s/%s/%s/%s' % (release, arch, bin, version)

def find_url(source, srcver, filename, release, arch):
    '''Try to find the archive URL for a given .deb filename'''
    url = None

    filepath, base = find_archive_path(source, release, arch, filename)

    return base_to_url(base,release,arch)

def scan_dsc_for_orig(filename):
    '''Parse a .dsc file for orig size and sum'''
    parser = apt_pkg.ParseTagFile(open_gpg_clearsign(filename))
    parser.Step()
    s = parser.Section

    for line in s['Files'].splitlines():
	sum, size, filename = line.strip().split()
        if filename.endswith('tar.gz'):
            return size, sum
    raise ValueError, "Cannot find tar.gz file in %s" % (filename)

def parse_changes(changes, info, cveset):
    '''Parse a .changes file, insert package information into the given info
    dictionary, and add the parsed CVEs to cveset.

    info has the following structure:

     release -> srcpkgname -> 'version' -> version,
                           -> 'binaries' -> set(),
                           -> 'arch' -> {arch -> {files -> (size,md5,url)}}
    '''

    # locate arch based on changes filename
    changes_arch = changes.split('_').pop().split('.')[0]
    if changes_arch not in official_architectures:
        return

    # parse .changes files into map s
    try:
        parser = apt_pkg.ParseTagFile(open_gpg_clearsign(changes))
        parser.Step()
        s = parser.Section
    except SystemError:
        print >>sys.stderr, 'ERROR: could not parse', changes
        raise

    release = s['Distribution']
    if '-' in release:
        release = releasemap[release]
    srcinfo = info.setdefault(release, {}).setdefault(s['Source'],{})
    srcinfo.setdefault('version', s['Version'])

    # Add binaries
    srcinfo.setdefault('binaries', set()).update(set(s['Binary'].split()))

    # Add files
    seen_tar = False
    for line in s['Files'].splitlines():
	sum, size, category, priority, filename = line.strip().split()
        # Skip translation bundles
        if filename.endswith('_translations.tar.gz'):
            continue
        if filename.endswith('deb'):
            name, version, arch = filename.split('_')
            arch, deb = arch.split('.')
        else:
            arch = 'source'
            if filename.endswith('tar.gz'):
                seen_tar = True
        files = srcinfo.setdefault('arch', {}).setdefault(arch,{})
	files.setdefault(filename, (size,sum,find_url(s['Source'],s['Version'],filename,release,arch)))

    # Special case for adding missing orig.tar.gz files
    if not seen_tar and changes.endswith('_source.changes'):
        filename = "%s_%s.orig.tar.gz" % (s['Source'],"-".join(s['Version'].split(':').pop().split('-')[:-1]))
        arch = "source"
        url = find_url(s['Source'], s['Version'], filename, release, arch)
	try:
            version = s['Version']
            if ':' in version and not version.endswith(':'):
                # strip out epoch, if it exists
                version = version[(version.find(':')+1):]
            size, sum = scan_dsc_for_orig('%s_%s.dsc' % (s['Source'], version))
        except:
            print >>sys.stderr, "orig.tar.gz not found in changes files, please specific dsc file with --dsc"
            raise
        files = srcinfo.setdefault('arch', {}).setdefault(arch,{})
        files.setdefault(filename, (size,sum,url))

    # update CVE set
    cveset.update(parse_CVEs(s['Changes']))


info = {}
CVEs = set([])

usn = args[0]

for changes in args[1:]:
    parse_changes(changes, info, CVEs)

local_pickle = os.path.join(config['usn_storage'], '%s.pickle' % (usn))
print '#!/bin/bash'
print 'set -e'
print 'export PATH=$PATH:%s' % (config['usn_tool'])
print 'export DB="--db %s"'%(local_pickle)
print 'export USN=%s'%(usn)
print 'umask 0002'
print 'echo Recording USN-$USN ...'
print '# Wipe out the old one -- we want to create a fresh one'
print 'rm -f %s'%(local_pickle)
print 'mkdir -p %s || true'%(config['usn_storage'])
print

possible_regression = False
if len(CVEs):
    print 'usn.py $DB $USN --cve ' + " --cve ".join(sorted(CVEs))
else:
    possible_regression = True
    print '# XXX FIX ME XXX No CVEs found!  Please include URL-based reference'
    print 'usn.py $DB $USN --cve "https://launchpad.net/bugs/XXXXXX"'
print

# Is this an updated CVE?
addition = False
origin = usn
if not usn.endswith('-1'):
    addition = True
    origin = usn.split('-')[0] + "-1"

srcs = set()
for release in info.keys():
    srcs.update(set(info[release].keys()))
title = ", ".join(sorted(srcs))
if len(CVEs)==1:
    title += " vulnerability"
else:
    title += " vulnerabilities"
summary = title

usn_db_tool = [os.path.join(config['usn_tool'],"usn.py"),'--db',config['usn_db_copy']]
if addition and possible_regression:
    # Fetch original CVE details
    title = subprocess.Popen(usn_db_tool + ['--show-title',origin], stdout=subprocess.PIPE).communicate()[0].rstrip()
    summary = subprocess.Popen(usn_db_tool + ['--show-summary',origin], stdout=subprocess.PIPE).communicate()[0].rstrip()
if addition:
    description = subprocess.Popen(usn_db_tool + ['--show-description',origin], stdout=subprocess.PIPE).communicate()[0].rstrip()
    action = subprocess.Popen(usn_db_tool + ['--show-action',origin], stdout=subprocess.PIPE).communicate()[0].rstrip()

print '# title: Email Subject, Web title.  summary: inside USN with package names'
print 'usn.py $DB $USN --title "%s" --summary "%s" --description - <<EOM' % (title,summary)
print 'XXX FILL ME IN XXX'
if addition:
    print 'XXX OR XXX'
    print 'USN-%s fixed vulnerabilities in XXX-APP-XXX. XXX FILL ME IN XXX' % origin
    print 'This update fixes the problem.'
    print '  XXX OR XXX'
    print 'This update provides the corresponding updates for XXX-APP-XXX.'
    print ''
    print 'XXX We apologize for the inconvenience.'
    print ''
    print 'Original advisory details:'
    print ''
    print ' ' + description.replace("\n","\n ")
print 'EOM'
print

print 'usn.py $DB $USN --action - <<EOM'
if addition:
    print action
    print 'XXX OR XXX'
print 'In general, a standard system upgrade is sufficient to effect the'
print 'necessary changes.'
print 'XXX OR XXX'
print 'After a standard system upgrade you need to restart XXX-APP-XXX to effect'
print 'the necessary changes.'
print 'XXX OR XXX'
print 'After a standard system upgrade you need to reboot your computer to'
print 'effect the necessary changes.'
print 'XXX OR IF MICROVERSIONEXCEPTION XXX'
print 'This update uses a new upstream release, which includes additional'
print 'bug fixes. In general, a standard system upgrade is sufficient to'
print 'effect the necessary changes.'
print 'XXX OR IF MICROVERSIONEXCEPTION XXX'
print 'This update uses a new upstream release, which includes additional'
print 'bug fixes. After a standard system upgrade you need to restart'
print 'XXX-APP-XXX to effect the necessary changes.'
print 'XXX MAYBE WITH XXX'
print '''ATTENTION: Due to an unavoidable ABI change the kernel updates have
been given a new version number, which requires you to recompile and
reinstall all third party kernel modules you might have installed. If
you use linux-restricted-modules, you have to update that package as
well to get modules which work with the new kernel version. Unless you
manually uninstalled the standard kernel metapackages (e.g. linux-generic,
linux-server, linux-powerpc), a standard system upgrade will automatically
perform this as well.'''
print 'EOM'
print

releases = []
for release in releasemap.values():
    if info.has_key(release):
        releases += [release]
releases.sort()

# Sources
for release in releases:
    for source in sorted(info[release].keys()):
        version = info[release][source]['version']
        print 'usn.py $DB $USN --release %s --package %s --source-version %s' % (release,source,version)
print

# Binaries
for release in releases:
    print '# Reduce to minimum binaries affected in %s' % (release)
    for source in sorted(info[release].keys()):
        version = info[release][source]['version']
	for deb in sorted(info[release][source]['binaries']):
            print '  usn.py $DB $USN --release %s --package %s --binary-version %s' % (release,deb,version)
	print

# URLs
print '# URLs'
print 'echo Recording URLs ...'
for release in releases:
    for source in sorted(info[release].keys()):
        for arch in official_architectures:
            if not info[release][source]['arch'].has_key(arch):
                continue
            for name in info[release][source]['arch'][arch]:
                size, md5, url = info[release][source]['arch'][arch][name]
		print 'usn.py $DB $USN --release %s --arch %s --url %s --url-size %s --url-md5 %s' % (release, arch, url, size, md5)
            print
        print
    print

print '# Update the central DB'
print 'echo Updating local master USN database ...'
print "usn.py $DB --export | usn.py --db %s --import" % (config['usn_db_copy'])
print 'echo Updating remote master USN database and sending template email ...'
print "usn.py $DB --export | ssh -C people.canonical.com '/home/ubuntu-security/bin/usn.sh --db /home/ubuntu-security/usn/database.pickle --import && /home/ubuntu-security/bin/send-usn-email '\"$USN\""

#import pprint
#pp = pprint.PrettyPrinter(indent=4)
#pp.pprint(info)
