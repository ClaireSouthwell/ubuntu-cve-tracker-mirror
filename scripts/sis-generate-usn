#!/usr/bin/env python
# Copyright 2007-2012 Canonical, Ltd.
# Authors: Kees Cook <kees@ubuntu.com>
#          Jamie Strandboge <jamie@ubuntu.com>
#          Marc Deslauriers <marc.deslauriers@ubuntu.com>
# License: GPLv2
# Goes and finds all the files that make up a set of .changes files.
#
# Example for linux package binary filter:
#  --filter-bins '^linux-image-\d'
import sys, os, os.path, tempfile, subprocess, re, time
import apt_pkg, gzip
import optparse
import cve_lib

import warnings
warnings.filterwarnings('ignore', message=r'.*apt_pkg\.TagFile.*', category=DeprecationWarning)

opter = optparse.OptionParser()
opter.add_option("--debug", help="Verbose processing output", action='store_true')
opter.add_option("--kernel-mode", help="Use special kernel mode", action='store_true')
opter.add_option("--no-new-warn", help="Do not warn about or error out on NEW binaries", action='store_true')
opter.add_option("--filter-bins", metavar="REGEX", help="Only include binary packages matching the REGEX in report", default=None)
opter.add_option("--cves", metavar="CVES", help="Comma separated list of CVEs to use instead of the normal *_source.changes autodetection (must be a superset).", default=None)
opter.add_option("--ignore-cves", metavar="CVES", help="Comma separated list of CVEs to ignore when doing CVE autodetection.", default=None)
(opt, args) = opter.parse_args()

if len(args) < 2:
    raise ValueError, "Must give USN and all changes files"

if opt.filter_bins:
    opt.filter_bins = re.compile(opt.filter_bins)
filter_dbg = re.compile("-dbg(sym)?$")

releasemap = dict()
for rel in cve_lib.releases:
    releasemap['%s-security' % (rel)] = rel

# Put -security first since it tends to have more correctly-validate
# components (-updates has gotten wrecked by things in -proposed before).
release_suffixes = ['','-security','-updates']

config = cve_lib.read_config()
archive = config['packages_mirror']
cve_lib.check_mirror_timestamp(config, mirror='packages_mirror')

pool = '%s/pool' % (archive)
dists = '%s/dists' % (archive)
published = 'http://security.ubuntu.com/ubuntu/pool/'
ports     = 'http://ports.ubuntu.com/pool/'

bin_locations = dict()
src_locations = dict()

if not os.path.exists(dists):
    print >>sys.stderr, "Could not find '%s'" % (dists)
    sys.exit(1)

for release in releasemap.values():
    if release in cve_lib.eol_releases:
        #print >>sys.stderr, "INFO: skipping EOL release '%s'" % (release)
        continue

    def scan_packages(locations, component, release, arch, name="Packages", di=False):
        '''Used to locate which component a given source package and
           related binary packages live'''
        archloc = arch
        if archloc != 'source':
            archloc = 'binary-%s' % (arch)
        # It seems that the d-i files have a separate name-space ('-di_*.udeb')
        # so it is safe to merge them together on a per-release basis.
        if di:
            dipath = 'debian-installer/'
        else:
            dipath = ""
        filename = '%s/%s/%s/%s%s/%s' % (dists, release, component, dipath, archloc, name)
        #print >>sys.stderr, "Loading %s" % (filename)
        if os.path.exists(filename):
            handle = open(filename)
        else:
            filename += ".gz"
            if not os.path.exists(filename):
                #print >>sys.stderr, "WARN: %s does not exist" % (filename)
                return
            handle = tempfile.TemporaryFile()
            handle.write(gzip.open(filename).read())
            handle.seek(0)
        #print >>sys.stderr, "INFO: loading %s" % (filename)
        parser = apt_pkg.ParseTagFile(handle)
        locations.setdefault(release, dict())
        locations[release].setdefault(arch, dict())
        while True:
            parser.Step()
            s = parser.Section
            if not s:
                break
            pkg = s['Package']
            pkgcomponent = component
            try:
                if '/' in s['Section']:
                    pkgcomponent = s['Section'].split('/')[0]
            except:
                pass
                #print >>sys.stderr, "%s(%s): " % (filename, pkg) + " ".join(s.keys())
                #raise
            if locations[release][arch].has_key(pkg):
                msg = "DEBUG: %s/%s/%s has multiple entries for '%s' (arch='%s')" % (release, pkgcomponent, archloc, pkg, s['Architecture'])
                # This is very noisy as NBS (not built from source) packages
                # are not autocleaned. This can happen for any number of
                # reasons. See LP: #988017 for details.
                #print >>sys.stderr, msg
            locations[release][arch].setdefault(pkg, pkgcomponent)

    for suffix in release_suffixes:
        for component in cve_lib.components:
            for arch in cve_lib.official_architectures:
                if arch == 'source' or arch == 'all':
                    continue
                scan_packages(bin_locations, component, release+suffix, arch)
                scan_packages(bin_locations, component, release+suffix, arch, di=True)
            scan_packages(src_locations, component, release+suffix, "source", name="Sources")

def open_gpg_clearsign(path):
    '''Open a GPG-signed .changes file and strip off the GPG envelope, so that
    it becomes suitable for apt_pkg parsing. Returns a file object.'''

    dest = tempfile.TemporaryFile()
    f = open(path)

    line = f.readline()
    clearsigned = False
    if line.startswith('-----BEGIN PGP SIGNED MESSAGE-----'):
        clearsigned = True
        for line in f:
            if line.strip() == '':
                break
    else:
        dest.write(line)

    for line in f:
        if clearsigned and line.find('BEGIN PGP SIGNATURE') >= 0:
            break
        dest.write(line)

    f.close()
    dest.seek(0)
    return dest

def parse_CVEs(text, cvelines):
    '''Return a set of all CVEs mentioned in the given text.'''

    # Looks for all CVE/CAN mentions
    result = set([])
    cvere = re.compile("((?:CAN|can|CVE|cve)-\d\d\d\d-\d\d\d\d)")
    for cve in cvere.finditer(text):
        result.add(cve.group().upper().replace('CAN', 'CVE', 1))

    # Record the lines the mentions came from
    for line in text.splitlines():
        line = line.rstrip()
        for cve in result:
            if cve in line:
                cvelines.setdefault(cve, [])
                cvelines[cve].append(line)

    return result

def find_archive_path_one_release(source, release, arch, filename, fuzzy=None):
    '''Try to find the local archive path for a given .deb filename, release, and architecture'''
    # When performing filename analysis for the component-match,
    # allow for a different filename (for handling guesses at NEW names).
    # This is distinct from "filename" so that the resulting archive path
    # contains the actual expected filename (we just do bin_location[]
    # look-ups with the "fuzzy" filename.
    if not fuzzy:
        fuzzy = filename

    if source.startswith('lib'):
        prefix = source[:4]
    else:
        prefix = source[:1]

    try:
        bin, version, archdeb = fuzzy.split('_',2)
    except:
        # non-deb: stored in the source package's location
        try:
            base = os.path.join(src_locations[release][arch][source], prefix, source, filename)
        except KeyError, e:
            raise KeyError, "cannot find %s path for %s %s/%s/%s" % (arch, release, prefix, source, filename)
        filepath = os.path.join(pool, base)
        return filepath, base

    # deb: stored in the binary package's location
    if arch == 'all':
        arch = 'i386'
    try:
        base = os.path.join(bin_locations[release][arch][bin], prefix, source, filename)
    except KeyError, e:
        failure = str(e)
        if failure == release:
            raise KeyError, "cannot find release '%s'" % (release)
        elif failure == arch:
            raise KeyError, "cannot find arch '%s' in release '%s'" % (arch, release)
        elif failure == bin:
            raise KeyError, "cannot find binary '%s' (arch '%s') in release '%s'" % (bin, arch, release)
        raise KeyError, "unknown KeyError '%s'" % (failure)
    filepath = os.path.join(pool, base)
    return filepath, base

def find_archive_path_each_release(source, release, arch, filename, fuzzy=None):
    '''Try each release to find an archive path'''
    msgs = []
    for suffix in release_suffixes:
        try:
            filepath, base = find_archive_path_one_release(source, release+suffix, arch, filename, fuzzy)
            return filepath, base
        except KeyError, e:
            msgs += [str(e)]
            pass
    raise KeyError, "\n".join(msgs)

def find_archive_path(source, release, arch, filename):
    try:
        filepath, base = find_archive_path_each_release(source, release, arch, filename)
    except KeyError, e:
        # Try to guess for an incremented NEW binary name
        found = False
        try:
            if filename.endswith('deb'):
                if opt.debug:
                    print >>sys.stderr, "missing: " + filename
                bin, version, archdeb = filename.split('_',2)
                # skip leading numbers (i.e. the kernel version)
                match = re.search(r'[^\d]+(?:-[\d\.]+)?-(\d+)', bin)
                if not match:
                    # Handle incrementing libraries (i.e. libdnsXX)
                    match = re.search(r'[^\d]+(\d+)', bin)
                if match:
                    if opt.debug:
                        print >>sys.stderr, "guessing abi: " + match.group(1)
                    inc = int(match.group(1)) - 1
                    while not found and inc > 0:
                        # Attempt to locate a binary with a similar name.
                        # I.e. if it ends with a number, find the same filename
                        # but with the number part reduced by 1.
                        newbin = match.string[:match.start(1)] + '%d' % (inc) + match.string[match.end(1):]
                        if opt.debug:
                            print >>sys.stderr, "trying: " + newbin
                        fuzzy_filename = "%s_%s_%s" % (newbin, version, archdeb)
                        try:
                            filepath, base = find_archive_path_each_release(source, release, arch, filename, fuzzy_filename)
                            found = True
                            if not opt.no_new_warn:
                                print >>sys.stderr, "WARN: found '%s' in place of NEW '%s'" % (newbin, bin)
                        except:
                            inc -= 1
                            pass
        except:
            pass
        if not found:
            if opt.no_new_warn:
                print >>sys.stderr, "INFO: new binary '%s' in release '%s'" % (filename,release)
            else:
                print >>sys.stderr, "ERROR: new binary '%s' in release '%s'" % (filename,release)
                raise e
            return '', ''

    return filepath, base

# Determine where to expect the files to live (archive vs ports)
def base_to_url(base, release, arch):
    if arch == 'lpia' or arch == 'armel' or arch == 'armhf':
        return os.path.join(ports,base)
    if arch in ['powerpc','sparc'] and release not in ['dapper','gutsy']:
        return os.path.join(ports,base)
    return os.path.join(published,base)

# This routine isn't useful for finding the expected download locations of debs
def lp_find_url(source, srcver, filename, release, arch):
    try:
        bin, version, archdeb = filename.split('_',2)
    except:
        return 'https://launchpad.net/ubuntu/%s/+source/%s/%s/+files/%s' % (release, source, srcver, filename)
    return 'https://launchpad.net/ubuntu/%s/%s/%s/%s' % (release, arch, bin, version)

def find_url(source, srcver, filename, release, arch):
    '''Try to find the archive URL for a given .deb filename'''
    url = None

    filepath, base = find_archive_path(source, release, arch, filename)

    return base_to_url(base,release,arch)

def scan_dsc_for_orig(filename):
    '''Parse a .dsc file for orig size and sum'''
    parser = apt_pkg.ParseTagFile(open_gpg_clearsign(filename))
    parser.Step()
    s = parser.Section

    for line in s['Files'].splitlines():
	sum, size, filename = line.strip().split()
        # TODO: adjust for multiple source tarballs as seen in:
        # http://wiki.debian.org/Projects/DebSrc3.0
        for type in ('gz', 'bz2', 'lzma', 'xz'):
            if filename.endswith('tar.' + type) and not filename.endswith('debian.tar.' + type):
                return size, sum, filename
    raise ValueError, "Cannot find source tar.* file in %s" % (filename)

def parse_changes(changes, info, cveset, cvelines):
    '''Parse a .changes file, insert package information into the given info
    dictionary, and add the parsed CVEs to cveset.

    info has the following structure:

     release -> srcpkgname -> 'version' -> version,
                           -> 'binaries' -> set(),
                           -> 'arch' -> {arch -> {files -> (size,md5,url)}},
                           -> 'description' -> description
    '''

    # locate arch based on changes filename
    changes_arch = changes.split('_').pop().split('.')[0]
    if changes_arch not in cve_lib.official_architectures:
        return

    # parse .changes files into map s
    try:
        parser = apt_pkg.ParseTagFile(open_gpg_clearsign(changes))
        parser.Step()
        s = parser.Section
    except SystemError:
        print >>sys.stderr, 'ERROR: could not parse', changes
        raise

    release = s['Distribution']
    if '-' in release:
        # This is a hack to pretend that changes from -proposed are
        # really coming through security. If we change releasemap
        # directly, we explode since it will pull multiple package
        # lists for the same release.
        if release.endswith('-proposed'):
            release = release.replace('-proposed','-security')
        release = releasemap[release]
    srcinfo = info.setdefault(release, {}).setdefault(s['Source'],{})
    srcinfo.setdefault('version', s['Version'])

    # Use hard-coded descriptions, or use the description for the binary
    # package name that matches the source package name, and if can't find
    # it, use # the first one.
    desc = cve_lib.description_overrides.get(s['Source'],"")
    if desc == "":
        for line in s['Description'].split('\n'):
            tmp_pkg = line.split(' - ')[0].strip()
            tmp_desc = " - ".join(line.split(' - ')[1:])
            if s['Source'] == tmp_pkg:
                desc = tmp_desc
                break
            elif desc == "":
                desc = tmp_desc
    srcinfo.setdefault('description', desc)

    # Add binaries
    srcinfo.setdefault('binaries', set()).update(set(s['Binary'].split()))

    # Add files
    seen_tar = False
    for line in s['Files'].splitlines():
	sum, size, category, priority, filename = line.strip().split()
        # Skip translation bundles
        if filename.endswith('_translations.tar.gz'):
            continue
        if filename.endswith('deb'):
            name, version, arch = filename.split('_')
            arch, deb = arch.split('.')
        else:
            arch = 'source'
            # TODO: adjust for multiple source tarballs as seen in:
            # http://wiki.debian.org/Projects/DebSrc3.0
            for type in ('gz', 'bz2', 'lzma'):
                if filename.endswith('tar.' + type) and not filename.endswith('debian.tar.' + type):
                    seen_tar = True
        files = srcinfo.setdefault('arch', {}).setdefault(arch,{})
	files.setdefault(filename, (size,sum,find_url(s['Source'],s['Version'],filename,release,arch)))

    # Special case for adding missing source tar.* files
    if not seen_tar and changes.endswith('_source.changes'):
	try:
            version = s['Version']
            if ':' in version and not version.endswith(':'):
                # strip out epoch, if it exists
                version = version[(version.find(':')+1):]
            size, sum, filename = scan_dsc_for_orig('%s_%s.dsc' % (s['Source'], version))
        except:
            print >>sys.stderr, "source tar.* not found in changes files, please specific dsc file with --dsc"
            raise

        # Build URL
        arch = "source"
        url = find_url(s['Source'], s['Version'], filename, release, arch)

        files = srcinfo.setdefault('arch', {}).setdefault(arch,{})
        files.setdefault(filename, (size,sum,url))

    # update CVE set
    if changes.endswith('_source.changes'):
        cveset.update(parse_CVEs(s['Changes'], cvelines))


info = {}
CVEs = set([])
cvelines = dict()

usn = args[0]

for changes in args[1:]:
    parse_changes(changes, info, CVEs, cvelines)

if opt.ignore_cves:
    CVEs.difference_update(set(opt.ignore_cves.split(',')))

if opt.cves:
    superset = set(opt.cves.split(','))
    if not superset.issuperset(CVEs):
        unexpected = CVEs.difference(superset)
        msgs = ["CVEs found in changelog but not command line: %s" % (" ".join(unexpected))]
        for cve in unexpected:
            msgs.append("\n".join(cvelines[cve]))
        raise ValueError, "\n".join(msgs)
    CVEs = superset

local_pickle = os.path.join(config['usn_storage'], '%s.pickle' % (usn))
print '#!/bin/bash'
print 'set -e'
print 'export PATH=$PATH:%s' % (config['usn_tool'])
print 'export DB="--db %s"'%(local_pickle)
print 'export USN=%s'%(usn)
print 'umask 0002'
print 'echo Recording USN-$USN ...'
print '# Wipe out the old one -- we want to create a fresh one'
print 'rm -f %s'%(local_pickle)
print 'mkdir -p %s || true'%(config['usn_storage'])
print

possible_regression = False
if len(CVEs):
    print 'usn.py $DB $USN --cve ' + " --cve ".join(sorted(CVEs))
else:
    possible_regression = True
    print '# XXX FIX ME XXX No CVEs found!  Please include URL-based reference'
    print 'usn.py $DB $USN --cve "https://launchpad.net/bugs/XXXXXX"'
print

# Is this an updated CVE?
addition = False
origin = usn
if not usn.endswith('-1'):
    addition = True
    origin = usn.split('-')[0] + "-1"

kernel_names = { 'linux': 'Linux kernel',
                 'linux-lts-backport-oneiric': 'Linux kernel (Oneiric backport)',
                 'linux-lts-backport-natty': 'Linux kernel (Natty backport)',
                 'linux-lts-backport-maverick': 'Linux kernel (Maverick backport)',
                 'linux-lts-quantal': 'Linux kernel (Quantal HWE)',
                 'linux-ec2': 'Linux kernel (EC2)',
                 'linux-fsl-imx51': 'Linux kernel (FSL-IMX51)',
                 'linux-ti-omap4': 'Linux kernel (OMAP4)',
                 'linux-mvl-dove': 'Linux kernel (Marvell DOVE)',
                 'linux-armadaxp': 'Linux kernel (Marvell Armada XP)' }

summary_extra = { 'linux-lts-quantal': ' - Linux kernel hardware enablement from Quantal' }

srcs = set()
for release in info.keys():
    srcs.update(set(info[release].keys()))
if opt.kernel_mode:
    title = ""
    for kernel_source in sorted(srcs):
        if title:
            title += ", "
        if kernel_source in kernel_names:
            title += kernel_names[kernel_source]
        else:
            title += "XXX Unknown kernel"
        summary = ", ".join(sorted(srcs))
        if kernel_source in summary_extra:
            summary += summary_extra[kernel_source]
else:
    title = ", ".join(sorted(srcs))
    summary = title

if len(CVEs)==1:
    title += " vulnerability"
    summary += " vulnerability"
else:
    title += " vulnerabilities"
    summary += " vulnerabilities"

usn_db_tool = [os.path.join(config['usn_tool'],"usn.py"),'--db',config['usn_db_copy']]
if addition and possible_regression:
    # Fetch original CVE details
    title = subprocess.Popen(usn_db_tool + ['--show-title',origin], stdout=subprocess.PIPE).communicate()[0].rstrip()
    summary = subprocess.Popen(usn_db_tool + ['--show-summary',origin], stdout=subprocess.PIPE).communicate()[0].rstrip()
if addition:
    description = subprocess.Popen(usn_db_tool + ['--show-description',origin], stdout=subprocess.PIPE).communicate()[0].rstrip()
    action = subprocess.Popen(usn_db_tool + ['--show-action',origin], stdout=subprocess.PIPE).communicate()[0].rstrip()

print '# title: used for Email Subject, Web title. XXX-EXPAND-TO-UPSTREAM-NAME-XXX'
print '# summary: used inside USN, should be package names'
print 'usn.py $DB $USN --title "%s" --summary "%s" --description - <<EOM' % (title,summary)
print 'XXX FILL ME IN: Detailed summary for admins XXX'
if len(CVEs):
    print subprocess.Popen(['%s/scripts/pull-usn-desc.py' % os.environ['UCT'], '--prioritize'] + sorted(CVEs), stdout=subprocess.PIPE).communicate()[0]
if not opt.kernel_mode:
    print 'XXX IF COMPILER PROTECTED XXX'
    print 'The default compiler options for affected releases should reduce the'
    print 'vulnerability to a denial of service.'
    print 'XXX OR IF APPARMOR PROTECTED XXX'
    print 'In the default installation, attackers would be isolated by the'
    print 'XXX-APP-XXX AppArmor profile.'
if addition:
    print 'XXX OR XXX'
    print 'USN-%s fixed vulnerabilities in XXX-APP-XXX. XXX FILL ME IN XXX' % origin
    print 'This update fixes the problem.'
    print '  XXX OR XXX'
    print 'This update provides the corresponding updates for XXX-APP-XXX.'
    print ''
    print 'XXX We apologize for the inconvenience.'
    print ''
    print 'Original advisory details:'
    print ''
    print ' ' + description.replace("\n","\n ")
print 'EOM'
print

print 'usn.py $DB $USN --issue-summary - <<EOM'
if opt.kernel_mode and len(CVEs) > 1:
    print 'Several security issues were fixed in the kernel.'
else:
    print 'XXX FILL ME IN: Summary for regular (non-admin) users XXX'
    print ''
    print 'XXX LOCAL TEMPLATES XXX'
    print 'XXX-APP-XXX could be made to crash or run programs as your login if it'
    print 'opened a specially crafted file.'
    print 'XXX OR XXX'
    print 'XXX-APP-XXX could be made to crash or run programs as an administrator'
    print 'if it opened a specially crafted file.'
    print 'XXX OR XXX'
    print 'XXX-APP-XXX could be made to crash if it opened a specially crafted'
    print 'file.'
    print 'XXX OR XXX'
    print 'XXX-APP-XXX could be made to crash if it received specially crafted'
    print 'input.'
    print 'XXX OR XXX'
    print 'XXX-APP-XXX could be made to overwrite files as the administrator.'
    print 'XXX OR XXX'
    print 'XXX-APP-XXX could be made to overwrite files.'
    print ''
    print 'XXX NETWORK TEMPLATES XXX'
    print 'XXX-APP-XXX could be made to crash or run programs as your login if it'
    print 'opened a malicious website.'
    print 'XXX OR XXX'
    print 'XXX-APP-XXX could be made to crash or run programs if it received'
    print 'specially crafted network traffic.'
    print 'XXX OR XXX'
    print 'XXX-APP-XXX could be made to crash or run programs if it received'
    print 'specially crafted network traffic from an authenticated user.'
    print 'XXX OR XXX'
    print 'XXX-APP-XXX would allow unintended access to files over the network.'
    print 'XXX OR XXX'
    print 'XXX-APP-XXX could be made to expose sensitive information over the'
    print 'network.'
    print 'XXX OR XXX'
    print 'Fraudulent security certificates could allow sensitive information to'
    print 'be exposed when accessing the Internet.'
    print ''
    print 'XXX KERNEL/PLUMBING TEMPLATES XXX'
    print 'The system could be made to crash under certain conditions.'
    print 'XXX OR XXX'
    print 'The system could be made to run programs as an administrator.'
    print 'XXX OR XXX'
    print 'The system could be made to crash or run programs as an administrator.'
    print 'XXX OR XXX'
    print 'The system could be made to crash if it received specially crafted'
    print 'network traffic.'
    print ''
    print 'XXX GENERIC TEMPLATES XXX'
    print 'Several security issues were fixed in XXX-APP-XXX.'

print 'EOM'
print

releases = []
for release in releasemap.values():
    if info.has_key(release):
        releases += [release]
releases.sort()

print '# Source descriptions for source packages. Defaults to what is found'
print '# in debian/control, but should be adjusted for readability. Templates'
print '# might only use one per source package, so in general it is best to'
print '# keep descriptions for the same source package across different'
print '# releases the same. XXX-CHECK-XXX'
for release in releases:
    for source in sorted(info[release].keys()):
        description = info[release][source]['description']
        print 'usn.py $DB $USN --release %s --package %s --source-description \'%s\'' % (release,source,description)
print

print 'usn.py $DB $USN --action - <<EOM'
if addition:
    print action
    print 'XXX OR XXX'
if opt.kernel_mode:
    print 'After a standard system update you need to reboot your computer to make'
    print 'all the necessary changes.'
    print 'XXX MAYBE WITH XXX'
    print '''ATTENTION: Due to an unavoidable ABI change the kernel updates have
been given a new version number, which requires you to recompile and
reinstall all third party kernel modules you might have installed. If
you use linux-restricted-modules, you have to update that package as
well to get modules which work with the new kernel version. Unless you
manually uninstalled the standard kernel metapackages (e.g. linux-generic,
linux-server, linux-powerpc), a standard system upgrade will automatically
perform this as well.'''
else:
    print 'In general, a standard system update will make all the necessary changes.'
    print 'XXX OR XXX'
    print 'After a standard system update you need to restart XXX-APP-XXX to make'
    print 'all the necessary changes.'
    print 'XXX OR XXX'
    print 'After a standard system update you need to reboot your computer to make'
    print 'all the necessary changes.'
    print 'XXX OR IF MICROVERSIONEXCEPTION XXX'
    print 'This update uses a new upstream release, which includes additional bug'
    print 'fixes. In general, a standard system update will make all the necessary'
    print 'changes.'
    print 'XXX OR IF MICROVERSIONEXCEPTION XXX'
    print 'This update uses a new upstream release, which includes additional bug'
    print 'fixes. After a standard system update you need to restart XXX-APP-XXX to'
    print 'make all the necessary changes.'

print 'EOM'
print


# Sources
for release in releases:
    for source in sorted(info[release].keys()):
        version = info[release][source]['version']
        print 'usn.py $DB $USN --release %s --package %s --source-version %s' % (release,source,version)
print

# Binaries
for release in releases:
    print '# Reduce to minimum binaries affected in %s' % (release)
    for source in sorted(info[release].keys()):
        version = info[release][source]['version']
        for deb in sorted(info[release][source]['binaries']):
            if not filter_dbg.search(deb) and (not opt.filter_bins or opt.filter_bins.search(deb)):
                print '  usn.py $DB $USN --release %s --package %s --binary-version %s' % (release,deb,version)
        print

# URLs
print '# URLs'
print 'echo Recording URLs ...'
for release in releases:
    for source in sorted(info[release].keys()):
        for arch in cve_lib.official_architectures:
            if not info[release][source]['arch'].has_key(arch):
                continue
            for name in info[release][source]['arch'][arch]:
                size, md5, url = info[release][source]['arch'][arch][name]
		print 'usn.py $DB $USN --release %s --arch %s --url %s --url-size %s --url-md5 %s' % (release, arch, url, size, md5)
            print
        print
    print

print '# Update the local DB'
print 'echo Updating local master USN database ...'
print "usn.py $DB --export | usn.py --db %s --import" % (config['usn_db_copy'])

print '# Check local DB for unfilled templates'
print '%s/scripts/check-unreplaced-templates.py %s $USN' % (os.environ['UCT'], config['usn_db_copy'])

print '# Update the remote DB'
print 'echo Updating remote master USN database and sending template email ...'
print "usn.py $DB --export | ssh -C people.canonical.com '/home/ubuntu-security/bin/usn.sh --db /home/ubuntu-security/usn/database-all.pickle --import && /home/ubuntu-security/bin/send-usn-email '\"$USN\""

#import pprint
#pp = pprint.PrettyPrinter(indent=4)
#pp.pprint(info)
